from __future__ import annotations

import os
from dotenv import load_dotenv

import json
import base64 
import logging
from typing import Dict, Any, Optional,List, Union, Iterable
from pydantic import BaseModel, Field, ValidationError

logger = logging.getLogger(__name__)

load_dotenv()  # Load environment variables from .env file

AI_PIPE_TOKEN = os.getenv("AI_PIPE_TOKEN")

class GeneratedFiles(BaseModel):
    """Schema for the files generated by the LLM.
    This class definition GeneratedFiles is a Pydantic model that defines
    a schema for the files generated by the LLM (Language Model).
    It has a single field named files, which is a dictionary mapping filenames
    (e.g., 'index.html', 'script.js') to their complete content as a string.
    The Field decorator is used to provide a description for the field."""
    # Reverting to the dictionary structure for simplicity
    files: Dict[str, str] = Field(
        description="A dictionary mapping filenames (e.g., 'index.html', 'script.js') to their complete content as a string."
    )


class GenerateCodeLLM:
    def __init__(self, model: Optional[str] = "openai/gpt-4.1-nano"):
        if not AI_PIPE_TOKEN:
            raise ValueError("AI_PIPE_TOKEN environment variable is required for LLM calls.")
        
        self.model = model
        self.token = AI_PIPE_TOKEN
        
        
    def _process_attachments(self, attachments: List[Dict[str, Any]]) -> str:
        """Decodes base64 attachments and formats them for the LLM prompt."""
        processed_data = []
        for attachment in attachments:
            url = attachment.get("url", "")
            name = attachment.get("name", "attachment")
            
            if url.startswith("data:"):
                try:
                    _, encoded_data = url.split(',', 1)
                except ValueError:
                    print(f"Error: Invalid data URI format for {name}")
                    continue

                try:
                    decoded_content = base64.b64decode(encoded_data).decode('utf-8')
                    sample_content = decoded_content[:1000] 
                    
                    processed_data.append(
                        f"## Attached File: {name}\n"
                        f"--- Content Sample ---\n"
                        f"{sample_content}\n"
                        f"----------------------\n"
                    )
                except Exception as e:
                    print(f"Error decoding attachment {name}: {e}")
        return "\n".join(processed_data)
    
    def generate_app_files(self, task_request: dict) -> Dict[str, str]:
        """Generates code files based on the task and returns them as a dictionary."""
        
        # 1. Prepare Data and Prompts
        attachments_context = self._process_attachments(task_request.get("attachments", []))
        brief = task_request.get("brief", "No detailed brief provided.")
        checks = task_request.get("checks", [])
        print(f"Generating files for brief: {brief} with checks: {checks}")
        # System Prompt enforces the role and output constraints (reverted to dictionary instruction)
        # print(attachments_context)

        user_prompt_text = (
            f"Generate all necessary files (at least index.html) to complete the following task:\n\n"
            f"**TASK BRIEF:** {brief}\n\n"
            f"**VERIFICATION CHECKS (for context):** {checks}\n\n"
            f"**ATTACHED DATA:**\n{attachments_context}\n\n"
            f"Ensure the generated code successfully meets the brief and is fully contained in the 'files' dictionary."
        )

        generated_files = self._call_llm(user_prompt_text, model=self.model)
        return generated_files

    def _call_llm(self,prompt: str, model: Optional[str] = "openai/gpt-4.1-nano") -> str:
        """Call an OpenAI-compatible chat completion using a proxy server AI PIPE and return text content.

        Requires AI_PIPE_TOKEN in environment.
        """
        token = os.getenv("AI_PIPE_TOKEN")
        if not token:
            raise ValueError("AI_PIPE_TOKEN environment variable is required for LLM calls.")
        import requests
        
        headers = {
        "Authorization": f"Bearer {self.token}",
        "Content-Type": "application/json"
        }
        
        url = "https://aipipe.org/openrouter/v1/chat/completions"
        
        system_prompt = (
                "You are an expert web developer specializing in generating clean, single-page "
                "web applications for simple tasks. Your output MUST be a single, valid JSON object "
                "that strictly adheres to the provided schema. The JSON object MUST contain a "
                "'files' dictionary where keys are the complete filenames (e.g., 'index.html', 'script.js') "
                "and values are the full content of those files as a string. "
                
                # --- ADDED REQUIREMENTS FOR EVALUATION COMPLIANCE ---
                "In addition to the main application files, you MUST ALWAYS include the following two files: "
                "1. **README.md**: A detailed README file specific to the application, describing its features and how to run it. "
                "2. **LICENSE**: The full and complete text of the MIT License. "
                
                # --- RETAINED STRICTNESS ---
                "\nDO NOT include any explanation, markdown formatting outside of the JSON block, or preamble."
            )
        
        payload = {
            "model": model,
            "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt},
                ]
        }
        print("Sending request to LLM...")
        try:
            response = requests.post(url, headers=headers, json=payload)
        except Exception as e:
            raise ValueError(f"LLM API call failed: {e}")

        print(f"LLM response status: {response.status_code}")
        if response.status_code != 200:
            raise ValueError(f"LLM API call failed: {response.status_code} {response.text}")

        # The LLM may return the JSON object as a string in message.content. Parse if needed.
        try:
            message_content = response.json()['choices'][0]['message']['content']
        except (ValueError, KeyError) as e:
            # ValueError from response.json() when body isn't JSON; KeyError if structure unexpected
            raise RuntimeError(f"Failed to parse LLM response JSON: {e}")

        # If the content is a JSON string, decode it into a Python dict first.
        if isinstance(message_content, str):
            try:
                parsed = json.loads(message_content)
            except json.JSONDecodeError:
                # Content wasn't valid JSON
                raise RuntimeError("LLM returned a non-JSON string in message.content")
        else:
            parsed = message_content

        # Validate against the GeneratedFiles schema and return the files dict
        try:
            validated_output = GeneratedFiles.model_validate(parsed)
        except ValidationError as e:
            raise RuntimeError(f"LLM returned invalid JSON structure or schema mismatch: {e}")
        
        return validated_output.files



    
    

